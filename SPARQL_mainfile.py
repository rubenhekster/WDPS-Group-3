# import traceback
# from pyspark.context import SparkContext
# import sys
# from nltk.tag import StanfordNERTagger
# import shutil
# from nltk.tokenize import word_tokenize
# import re
# sys.path.append('/home/wdps1703/lib/python2.7/site-packages')
# import requests
# import json
# import collections, math

# def ner_stanford((x, text), st):
#     tokenized_text = word_tokenize(text.decode('UTF-8'))
#     classified_text = st.tag(tokenized_text)
#     output = []
#     for tup in classified_text:
#         if tup[1] != 'O':
#             output.append(tup)
#     return ((x,output))

# # defines which tags are excluded from the HTML file
# def visible(element):
#     if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:
#         return False
#     elif re.match('<!--.*-->', element.encode('UTF-8')):
#         return False
#     return True


# def decode(x, record_attribute):
#     html_pages_array = []
#     _, payload = x
#     wholeTextFile = ''.join([c.encode('utf-8') for c in payload])
#     wholeTextFile = "WARC/1.0 " + wholeTextFile

#     from cStringIO import StringIO
#     from warcio.archiveiterator import ArchiveIterator
#     from html2text import HTML2Text
#     from bs4 import BeautifulSoup
#     stream = StringIO(wholeTextFile)
#     try:
#         for record in ArchiveIterator(stream):
#             # if the record type is a response (which is the case for html page)

#                 if record.rec_type == 'response':
#                     # check if the response is http
#                     if record.http_headers != None:
#                         # Get the WARC-RECORD-ID
#                         record_id = record.rec_headers.get_header(record_attribute)
#                         # Clean up the HTML using BeautifulSoup
#                         html = record.content_stream().read()
#                         soup = BeautifulSoup(html, "html5lib")
#                         data = soup.findAll(text=True)
#                         result = filter(visible, data)
#                         result2 = ' '.join(result)
#                         result2 = ' '.join(result2.split()).encode('utf-8')
#                         # Build up the resulting list.
#                         # result2 = re.sub(r'[\?\.\!]+(?=[\?\.\!])', '.', result2)
#                         html_pages_array.append((record_id, result2))
#     except Exception:
#         print("Something went wrong with the archive entry")

#     return html_pages_array

# def create_output((x, text)):
#     output = []
#     text = list(set(text))
#     for (entity, tag) in text:
#         output.append(str(x)+","+entity+","+tag)
#     return output


# record_attribute = sys.argv[1]  # "WARC-Record-ID"
# in_file = sys.argv[2]  # "/home/kevin/Documents/WDPS/wdps2017/CommonCrawl-sample.warc.gz"
# stanford = sys.argv[3] # https://nlp.stanford.edu/software/stanford-ner-2017-06-09.zip
# # We read one WARC file. This list will contain tuples consisting of the WARC-Record-ID and the cleaned up HTML

# # Create Spark Context -- Remove this when running on cluster
# sc = SparkContext.getOrCreate()
# st = StanfordNERTagger(stanford + '/classifiers/english.all.3class.distsim.crf.ser.gz',
#                        stanford + '/stanford-ner.jar',
#                            encoding='utf-8')

# rdd_whole_warc_file = rdd = sc.newAPIHadoopFile(in_file,
#                                                 "org.apache.hadoop.mapreduce.lib.input.TextInputFormat",
#                                                 "org.apache.hadoop.io.LongWritable",
#                                                 "org.apache.hadoop.io.Text",
#                                                 conf={"textinputformat.record.delimiter": "WARC/1.0"})

# rdd_html_cleaned = rdd_whole_warc_file.flatMap(lambda x: decode(x, record_attribute))

# stanford_rdd = rdd_html_cleaned.map(lambda (x, y): ner_stanford((x, y), st))
# # Extract named entities
# text_rdd = stanford_rdd.flatMap(lambda (x,y):create_output((x,y)))

# print(text_rdd.collect())

ELASTICSEARCH_URL = 'http://10.149.0.127:9200/freebase/label/_search'
TRIDENT_URL = 'http://10.141.0.11:8082/sparql'

query = 'obama' # token obtained 

print('Searching for "%s"...' % query)
#looking for queries that we get from the token with elasticsearch
response = requests.get(ELASTICSEARCH_URL, params={'q': query, 'size':100})

print(response)
#select unique query results 
ids = set()
labels = {}
scores = {}

#obtain freebase id's from elasticsearch responses
if response:
    response = response.json()
    for hit in response.get('hits', {}).get('hits', []):
        freebase_id = hit.get('_source', {}).get('resource')
        label = hit.get('_source', {}).get('label')
        score = hit.get('_score', 0)
        ids.add( freebase_id )
        scores[freebase_id] = max(scores.get(freebase_id, 0), score)
        labels.setdefault(freebase_id, set()).add( label )

print('Found %s results.' % len(labels))

#predixes to use shortnames in SPARQL query 
prefixes = """
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX owl: <http://www.w3.org/2002/07/owl#>
PREFIX fbase: <http://rdf.freebase.com/ns/>
PREFIX wd: <http://www.wikidata.org/entity/>
PREFIX wds: <http://www.wikidata.org/entity/statement/>
PREFIX wdv: <http://www.wikidata.org/value/>
PREFIX wdt: <http://www.wikidata.org/prop/direct/>
"""
### look at NER  tag. if person filter with personEntity, if location filter with locationEntity if organisation
#filter with organisation entity

#select person entity 
personEntity_same_as_template = prefixes + """
SELECT DISTINCT ?person 
WHERE
{
	?person wdt:P31 wd:Q5 .       #where ?person isa(wdt:P31) human(wd:Q5)
	?s owl:sameAs %s .
    	{ ?s owl:sameAs ?person .} UNION { ?person owl:sameAs ?s .}
}
"""

#select organisation enitity 

organisationEntity_same_as_template = prefixes + """
SELECT DISTINCT ?organisation ?organisation2 
WHERE 
{
  ?organisation wdt:P31 wd:Q43229. #organisation(Q43229) (collective goal)
  ?organisation2 wdt:P31 wd:Q2029841. #organisation(Q2029841) (economical concept)
  ?s owl:sameAs %s .
  { ?s owl:sameAs ?organisation . OR ?s owl:sameAs ?organisation . } UNION { ?organisation  owl:sameAs ?s . OR ?organisation2  owl:sameAs ?s .}
}
"""

#select location enitity 

locationEntity_same_as_template = prefixes + """
SELECT DISTINCT ?location 
WHERE 
{
  ?location wdt:P31 wd:Q17334923. #where ?location isA location(Q17334923)
  ?s owl:sameAs %s .
  { ?s owl:sameAs ?location .} UNION { ?location owl:sameAs ?s .}
}
"""


#get the word similar to the freebase hit %s
same_as_template = prefixes + """
SELECT DISTINCT ?same 
WHERE 
{
    ?s owl:sameAs %s .
    { ?s owl:sameAs ?same .} UNION { ?same owl:sameAs ?s .}
}
"""
# get the complete template for the freebase hit %s
po_template = prefixes + """
SELECT DISTINCT * WHERE {
    %s ?p ?o.
}
"""

print('Counting KB facts...')
#Link all results from elasticsearch to trident database.  %s in po_templare (are the unique freebase hits)  
facts  = {}
n_total = 0
for i in ids:
    response = requests.post(TRIDENT_URL, data={'print': False, 'query': po_template % i})
    if response:
         response = response.json()
         n = int(response.get('stats',{}).get('nresults',0))
         print(i, ':', n)
         sys.stdout.flush()
         facts[i] = n
	  n_total = n_total+n
 
def get_best(i):
    return math.log(facts[i]) * scores[i]

#best matches are filtered based on the entity type
print('Best matches:')
for i in sorted(ids, key=get_best, reverse=True)[:3]:
    print(i, ':', labels[i], '(facts: %s, score: %.2f)' % (facts[i], scores[i]) )
    
    # the normalized score, which we will use when ranking the obtained entities
    norm_score = facts[i]/n_total
    
    sys.stdout.flush()
	#look which entity it is to choose the suited SPARQL query , tag = NER tag 
	if tag == PERSON:
      	    response = requests.post(TRIDENT_URL, data={'print': True, 'query': personEntity_same_as_template % i})
	    if response:
		response = response.json()
		for binding in response.get('results', {}).get('bindings', []):
		    print(' =', binding.get('same', {}).get('value', None))
		
	elif tag == ORGANISATION:
	    response = requests.post(TRIDENT_URL, data={'print': True, 'query': organisationEntity_same_as_template % i})
	    if response:
		response = response.json()
		for binding in response.get('results', {}).get('bindings', []):
		    print(' =', binding.get('same', {}).get('value', None))
		
	elif tag == LOCATION:
	    response = requests.post(TRIDENT_URL, data={'print': True, 'query': locationEntity_same_as_template % i})
	    if response:
		response = response.json()
		for binding in response.get('results', {}).get('bindings', []):
		    print(' =', binding.get('same', {}).get('value', None))
	else:
	    response = requests.post(TRIDENT_URL, data={'print': True, 'query': same_as_template % i})
	    if response:
		response = response.json()
		for binding in response.get('results', {}).get('bindings', []):
		    print(' =', binding.get('same', {}).get('value', None))

